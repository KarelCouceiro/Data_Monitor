{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVaDx4yxFomA5NxV3Soab4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarelCouceiro/Data_Monitor/blob/main/LLM_runpod_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "4CYmIYmjNwdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv0TBhfaNsgl",
        "outputId": "cc2de96a-9937-4002-d872-f1b35196cf9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Merge sorting is not a built-in function in Teradata, but it can be implemented using a combination of common table expressions (CTEs) and window functions. Here's an example of how you could implement a merge sort algorithm in Teradata:\n",
            "\n",
            "1. Define a CTE to hold the left and right halves of the sorted data:\n",
            "```sql\n",
            "WITH LEFT_HALF AS (\n",
            "    SELECT * FROM TABLE_NAME WHERE COLUMN_NAME = 'left_value'\n",
            "), RIGHT_HALVE AS (\n",
            "    SELECT * FROM TABLE_NAME WHERE COLUMN_NAME = 'right_value'\n",
            ")\n",
            "SELECT * INTO SORTED_TABLE FROM LEFT_HALF\n",
            "MERGE JOIN RIGHT_HALVE ON LEFT_HALVE.COLUMN_NAME = RIGHT_HALVE.COLUMN_NAME;\n",
            "```\n",
            "In this example, `LEFT_HALF` and `RIGHT_HALVE` are CTEs that hold the left and right halves of the sorted data, respectively. The `MERGE JOIN` clause combines the two CTEs based on the specified column name, effectively merging the two halves into a single sorted table.\n",
            "\n",
            "2. Implement the merge sort logic within the CTEs:\n",
            "```sql\n",
            "WITH LEFT_HALF AS (\n",
            "    SELECT * FROM TABLE_NAME WHERE COLUMN_NAME = 'left_value'\n",
            "), RIGHT_HALVE AS (\n",
            "    SELECT * FROM TABLE_NAME WHERE COLUMN_NAME = 'right_value'\n",
            ")\n",
            "SELECT * INTO SORTED_TABLE FROM LEFT_HALF\n",
            "ORDER BY COLUMN_NAME;\n",
            "\n",
            "IF ODD(COUNT(*) - 1) THEN\n",
            "    INSERT INTO RIGHT_HALVE (SELECT * FROM LEFT_HALVE ORDER BY COLUMN_NAME);\n",
            "    LEFT_HALVE = CONCAT('LEFT', LEFT_HALVE);\n",
            "ELSE IF EVEN(COUNT(*) - 1) THEN\n",
            "    INSERT INTO LEFT_HALVE (SELECT * FROM RIGHT_HALVE ORDER BY COLUMN_NAME);\n",
            "    RIGHT_HALVE = CONCAT('RIGHT', RIGHT_HALVE);\n",
            "END IF;\n",
            "```\n",
            "In this example, the `ODD` and `EVEN` functions are used to determine whether the number of rows in the current half is odd or even. If the number of rows is odd, the `INSERT` statement inserts the rows from the other half into the current half, and the `CONCAT` function concatenates the two halves into a single sorted table. If the number of rows is even, the process is repeated for the other half.\n",
            "\n",
            "3. Repeat steps 1-2 until the desired level of sorting is achieved:\n",
            "```sql\n",
            "WITH LEFT_HALF AS (\n",
            "    SELECT * FROM TABLE_NAME WHERE COLUMN_NAME = 'left_value'\n",
            "), RIGHT_HALVE AS (\n",
            "    SELECT * FROM TABLE_NAME WHERE COLUMN_NAME = 'right_value'\n",
            ")\n",
            "SELECT * INTO SORTED_TABLE FROM LEFT_HALVE\n",
            "ORDER BY COLUMN_NAME;\n",
            "\n",
            "IF ODD(COUNT(*) - 1) THEN\n",
            "    INSERT INTO RIGHT_HALVE (SELECT * FROM LEFT_HALVE ORDER BY COLUMN_NAME);\n",
            "    LEFT_HALVE = CONCAT('LEFT', LEFT_HALVE);\n",
            "ELSE IF EVEN(COUNT(*) - 1) THEN\n",
            "    INSERT INTO LEFT_HALVE (SELECT * FROM RIGHT_HALVE ORDER BY COLUMN_NAME);\n",
            "    RIGHT_HALVE = CONCAT('RIGHT', RIGHT_HALVE);\n",
            "END IF;\n",
            "\n",
            "REPEAT UNTIL COUNT(*) = 0;\n",
            "```\n",
            "In this example, the process is repeated until the desired level of sorting is achieved, as indicated by the `UNTIL` clause. The `COUNT(*)` function is used to check whether there are any remaining rows to sort.\n",
            "\n",
            "Note that this implementation assumes that the `COLUMN_NAME` value is constant for all rows in the table. If this is not the case, you may need to modify the CTEs and merge join clause accordingly. Additionally, this implementation does not handle edge cases such as an empty left or right half, which may require additional logic to handle correctly.\n"
          ]
        }
      ],
      "source": [
        "url = \"https://api.runpod.ai/v2/1e7880bd8ki4ks/runsync\"\n",
        "\n",
        "payload = json.dumps({\n",
        "  \"input\": {\n",
        "    \"prompt\": \"<<SYS>>You are a helpful, respectful and honest assistant. Always answer as helpfully as possible. If you don't know the answer to a question, please don't share false information.<</SYS>>[INST]User: How to write in SQL the merge sort algorithm in Teradata[/INST]\",\n",
        "    \"max_new_tokens\": 2500,\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.7,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"batch_size\": 8,\n",
        "    \"stop\": [\n",
        "      \"</s>\"\n",
        "    ]\n",
        "  }\n",
        "})\n",
        "headers = {\n",
        "  'Content-Type': 'application/json',\n",
        "  'Authorization': 'Bearer ULLKF1WNY5Y8Z5FU4MGACHBBQSPESHXDOEOMBBW7'\n",
        "}\n",
        "\n",
        "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "response_text = json.loads(response.text)\n",
        "\n",
        "print(response_text['output'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "payload = json.dumps({\n",
        "  \"input\": {\n",
        "    \"prompt\": \"<<SYS>>You are an expert matter communicatiing with SQL developers at advance level of abstraction<</SYS>>[INST]User: How to write in SQL the merge sort algorithm in Teradata[/INST]\",\n",
        "    \"max_new_tokens\": 10000,\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.7,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"batch_size\": 8,\n",
        "    \"stop\": [\n",
        "      \"</s>\"\n",
        "    ]\n",
        "  }\n",
        "})"
      ],
      "metadata": {
        "id": "VhJbzFrybmYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "response_text = json.loads(response.text)\n",
        "\n",
        "print(response_text['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLYvEf9ebxop",
        "outputId": "f2030331-a6b4-4004-9211-52ff131ff9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  As an expert matter communicator, I'll provide a high-level overview of how to implement the merge sort algorithm in SQL for Teradata. Please note that this is not a comprehensive implementation, but rather a simplified example to illustrate the basic principles.\n",
            "\n",
            "1. Define the tables and columns:\n",
            "Create two temporary tables, `tab1` and `tab2`, with the same schema as your original table. These tables will be used to store the left and right halves of the sorted data during the merge process.\n",
            "```sql\n",
            "CREATE TEMPORARY TABLE tab1 (\n",
            "    id INT PRIMARY KEY,\n",
            "    column1 VARCHAR(50),\n",
            "    column2 DATE,\n",
            "    -- Add any other relevant columns here\n",
            ");\n",
            "\n",
            "CREATE TEMPORARY TABLE tab2 (\n",
            "    id INT PRIMARY KEY,\n",
            "    column1 VARCHAR(50),\n",
            "    column2 DATE,\n",
            "    -- Add any other relevant columns here\n",
            ");\n",
            "```\n",
            "2. Split the input table into two halves:\n",
            "Write a query to split the original table into two equal parts based on some criteria (e.g., column value). For example:\n",
            "```sql\n",
            "SELECT * FROM mytable WHERE column = 'value1' INTO tab1;\n",
            "SELECT * FROM mytable WHERE column = 'value2' INTO tab2;\n",
            "```\n",
            "3. Merge the two halves:\n",
            "Write a series of queries to merge the two halves into a single, sorted table. The basic idea is to compare rows from the two halves and place the smaller one in the correct position in the final table. Here's an example of how to do this using a self-join:\n",
            "```sql\n",
            "-- Step 1: Sort the two halves by the comparison column\n",
            "SELECT * FROM tab1 ORDER BY column1;\n",
            "SELECT * FROM tab2 ORDER BY column1;\n",
            "\n",
            "-- Step 2: Self-join the two sorted tables\n",
            "SELECT t1.*, t2.*, ROW_NUMBER() OVER (ORDER BY column1) AS row_num\n",
            "FROM tab1 t1\n",
            "JOIN tab2 t2 ON t1.column1 = t2.column1;\n",
            "\n",
            "-- Step 3: Drop the redundant columns and sort the final table\n",
            "SELECT * FROM (\n",
            "    SELECT t1.*, t2.*, ROW_NUMBER() OVER (ORDER BY column1) AS row_num\n",
            "    FROM tab1 t1\n",
            "    JOIN tab2 t2 ON t1.column1 = t2.column1\n",
            ")\n",
            "WHERE row_num = 1;\n",
            "```\n",
            "4. Clean up:\n",
            "Drop the temporary tables `tab1` and `tab2`.\n",
            "```sql\n",
            "DROP TABLE tab1;\n",
            "DROP TABLE tab2;\n",
            "```\n",
            "This is just a high-level overview of how to implement the merge sort algorithm in SQL for Teradata. Depending on your specific use case, you may need to modify the query or add additional steps. However, this should give you a good starting point for understanding the basic principles of the merge sort algorithm in SQL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_question(question, system):\n",
        "    payload = {\n",
        "        \"input\": {\n",
        "            \"prompt\": \"<<SYS>>{{SYSTEM}}<</SYS>>[INST]User: {{QUESTION}}[/INST]\",\n",
        "            \"max_new_tokens\": 10000,\n",
        "            \"temperature\": 0.9,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 0.7,\n",
        "            \"repetition_penalty\": 1.2,\n",
        "            \"batch_size\": 8,\n",
        "            \"stop\": [\n",
        "                \"</s>\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    payload[\"input\"][\"prompt\"] = payload[\"input\"][\"prompt\"].replace(\"{{QUESTION}}\", question).replace(\"{{SYSTEM}}\", system)\n",
        "    json_payload = json.dumps(payload)\n",
        "    return json_payload"
      ],
      "metadata": {
        "id": "XmazehJogpMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys = \"You are an expert which can provide a summary in English in three bullet point with emojis ilustrating the main ideas\"\n",
        "quest = '''Do you want to gain a deeper understanding of your models and better mitigate poor prediction risks\n",
        "associated with machine learning interpretation? If so, then Interpretable Machine Learning with Python deserves a\n",
        "place on your bookshelf.We'll be starting off with the fundamentals of interpretability, its relevance in business,\n",
        "and exploring its key aspects and challenges. As you progress through the chapters, you'll then focus on how white-box models work,\n",
        "compare them to black-box and glass-box models, and examine their trade-off. You'll also get you up to speed with a vast array of interpretation methods,\n",
        "also known as Explainable AI (XAI) methods, and how to apply them to different use cases, be it for classification or regression, for tabular, time-series,\n",
        "image or text. In addition to the step-by-step code, this book will also help you interpret model outcomes using examples. You'll get hands-on with tuning\n",
        "models and training data for interpretability by reducing complexity, mitigating bias, placing guardrails, and enhancing reliability.\n",
        "The methods you'll explore here range from state-of-the-art feature selection and dataset debiasing methods to monotonic constraints and adversarial retraining.\n",
        "By the end of this book, you'll be able to understand ML models better and enhance them through interpretability tuning.'''\n",
        "\n"
      ],
      "metadata": {
        "id": "yrRotx1Xhu-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.request(\"POST\", url, headers=headers, data=replace_question(question=quest, system=sys))\n",
        "\n",
        "response_text = json.loads(response.text)\n",
        "\n",
        "print(response_text['output'])"
      ],
      "metadata": {
        "id": "aSABt_Q5uBuS",
        "outputId": "533296d0-e213-4acd-bb0b-b30445ad81ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Sure! Here is a summary of the book \"Interpretable Machine Learning with Python\" in three bullet points with emojis illustrating the main ideas:\n",
            "\n",
            "🔍 Understand the Fundamentals of Interpretability: The book covers the importance of interpretability in machine learning, including its relevance in business and the key aspects and challenges of interpreting models.\n",
            "\n",
            "💡 Explore Explainable AI Methods: The book provides a comprehensive overview of various explanation methods, such as feature importance, partial dependence plots, and SHAP values, and shows how to apply these methods to different types of datasets and use cases.\n",
            "\n",
            "📊 Tune Models for Interpretability: The book teaches readers how to tune their models for interpretability by reducing complexity, mitigating bias, placing guardrails, and enhancing reliability. This includes techniques such as feature selection, dataset debiasing, and adversarial retraining. By the end of the book, readers will have a deeper understanding of their models and be able to make more informed decisions based on their predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest ='''Select Table1.UID\n",
        "FROM Table1 R\n",
        "\n",
        "INNER JOIN Table2 T ON\n",
        "R.UID = T.UID\n",
        "\n",
        "INNER JOIN Table3 E ON\n",
        "T.PARENT = E.PARENT\n",
        "AND E.UID NOT IN (SELECT UID FROM Table1)'''\n",
        "\n",
        "sys = \"You are an expert SQL developer which produce optimized SQL queries\""
      ],
      "metadata": {
        "id": "1qQn8Z0l0Msx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.request(\"POST\", url, headers=headers, data=replace_question(question=quest, system=sys))\n",
        "\n",
        "response_text = json.loads(response.text)\n",
        "\n",
        "print(response_text['output'])"
      ],
      "metadata": {
        "id": "SMJcFpMh0dbc",
        "outputId": "34b79c33-1c8e-4a66-d5a4-2e4fd4bec9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Your query is almost optimal, but there are a few things you could do to improve it further:\n",
            "\n",
            "1. Use explicit joins instead of using the comma-separated list of tables in the FROM clause. This can make the query easier to read and maintain.\n",
            "2. Use meaningful table aliases for the joins to make the query more readable. For example, you could use `t` for `Table2`, `e` for `Table3`, and `r` for `Table1`.\n",
            "3. Use the `JOIN` keyword instead of the `,` operator to separate the joins. This can make the query more readable and reduce the risk of errors.\n",
            "4. Use the `LEFT JOIN` keyword instead of the `INNER JOIN` keyword if you want to include all rows from `Table1` and `Table2`, even if there are no matching rows in `Table3`.\n",
            "5. Use the `IS NOT NULL` predicate instead of the `NOT IN` clause to filter out the `UID` values that exist in `Table1`. This can be more efficient and avoids the need to perform a subquery.\n",
            "6. Consider adding an index on the `UID` column in `Table1` and `Table2` to speed up the query.\n",
            "7. Consider using a more specific join condition instead of the `AND` clause with multiple conditions. For example, you could use `T.UID = R.UID AND E.UID = R.UID` to avoid joining tables that have no matching rows.\n",
            "8. Consider using a more specific filter condition instead of the `AND` clause with multiple conditions. For example, you could use `E.UID IS NOT NULL` to filter out the `UID` values that exist in `Table1`.\n",
            "\n",
            "Here's the optimized version of your query:\n",
            "```sql\n",
            "SELECT t1.UID\n",
            "FROM Table1 t1\n",
            "INNER JOIN Table2 t2 ON t1.UID = t2.UID\n",
            "INNER JOIN Table3 e ON t2.PARENT = e.PARENT AND e.UID IS NOT NULL;\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define the number of rows to generate\n",
        "num_rows = 10000\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Loop through the number of rows to generate\n",
        "for i in range(num_rows):\n",
        "    # Generate a random policy ID\n",
        "    policy_id = random.randint(10001, 20000)\n",
        "\n",
        "    # Generate a random customer ID\n",
        "    customer_id = random.randint(2001, 3000)\n",
        "\n",
        "    # Generate a random state\n",
        "    state = random.choice([\"CA\", \"NY\", \"TX\", \"FL\"])\n",
        "\n",
        "    # Generate a random deductible\n",
        "    deductible = random.randint(1000, 3000)\n",
        "\n",
        "    # Generate a random premium\n",
        "    premium = random.randint(1200, 3600)\n",
        "\n",
        "    # Generate a random claim amount\n",
        "    claim_amount = random.randint(100, 600)\n",
        "\n",
        "    # Create a dictionary to store the data for each row\n",
        "    row_data = {\n",
        "        \"policyID\": policy_id,\n",
        "        \"customerID\": customer_id,\n",
        "        \"state\": state,\n",
        "        \"deductible\": deductible,\n",
        "        \"premium\": premium,\n",
        "        \"claimAmount\": claim_amount\n",
        "    }\n",
        "\n",
        "    # Append the row data to the list of data\n",
        "    data.append(row_data)\n",
        "df = pd.DataFrame(data)\n",
        "# Print the data\n",
        "print(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VAXSYZBSehd",
        "outputId": "42789b4a-e69f-40db-df31-d3e102874c35"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      policyID  customerID state  deductible  premium  claimAmount\n",
            "0        16362        2819    TX        2510     2009          111\n",
            "1        14383        2531    NY        2939     2079          360\n",
            "2        16075        2850    FL        1363     2933          104\n",
            "3        17448        2069    FL        2714     1874          179\n",
            "4        18604        2394    CA        2206     2481          323\n",
            "...        ...         ...   ...         ...      ...          ...\n",
            "9995     11930        2347    FL        1485     3395          413\n",
            "9996     13380        2599    TX        2334     2501          228\n",
            "9997     12975        2770    CA        2632     1294          102\n",
            "9998     14110        2416    FL        2887     3352          313\n",
            "9999     18049        2453    NY        2744     1826          260\n",
            "\n",
            "[10000 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.express import px"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "7saFJiABBC-Y",
        "outputId": "8f133801-4790-41d3-96be-b1665c04ac16"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-344dafa258cb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'px' from 'plotly.express' (/usr/local/lib/python3.10/dist-packages/plotly/express/__init__.py)"
          ]
        }
      ]
    }
  ]
}